Primeiro crio um projeto dentro do chat e adquiro uma key de uma api de dentro do openai

crio uma pasta src/index.ts

coloco essa API KEY dentro de um dotenv onde eu chamarei essa key no index.ts

importanto o dotenv e apos instalação da biblioteca openai eu tbm faço a importação da mesma:

import dotenv from "dotenv"
import OpenAI from "openai"
dotenv.config();

const client = new OpenAI({
    apiKey: process.env.OPEN_AI_SECRET_API_KEY
})

client.chat.completions.create({  --> retorna uma promisse ou seja conseguimos usar o .then((response) => {})
    model: 'gpt-4o-mini',
    max_completions_tokens: 100,
    temperature: 0,
    top_p: 0.1,
    messages: [
        {
            role: "developer", --> mensagem do dev 
            content: "User emogis a cada 2 palavras"
        },
        {
            role: "user", --> mensagem do usuario
            content: "Escreva uma mensagem de uma frase sobre unicornios"
        },
        {
            role: "assistant", --> mensagem que o chat respondeu
            content: "//mensagem digitada pelo chat"
        },
        {
            role: "user", --> mensagem do usuario
            content: "Obrigado"
        },
    ]
}).then((completion // or response) => {
    console.log(completion.choices[0].message.content)
})

console.log(completion.choices[0].message.content) --> retorna um array de choices que são varias respostas

Parametros da API:
    model: modelo de chat que quero usar
    menssages: um array que possui varias mensagens onde cada mensagem vai tem o 
        conteudo: content
        role: quem esta enviando a mensagem temos 3: user, developer, assistant
    max_completions_tokens: quantidade maxima de tokens da mensagem a enviar, bom para limitar gastos
    temperature: nivel de determinismo/criativo quanto mais alto mais criativo de 0 a 2
    top_p: O modelo prevê uma lista de palavras candidatas, cada uma com uma probabilidade

Arrumando tudo:

import dotenv from "dotenv"
import OpenAI from "openai"
dotenv.config();

const client = new OpenAI({
    apiKey: process.env.OPEN_AI_SECRET_API_KEY
})

function async generateText() {

    const completion = await client.chat.completions.create({
        model: "gpt-4o-mini",
        max_completions_tokens: 100,
        messages: [
            {
                role: "developer", --> mensagem do dev 
                content: "User emogis a cada 2 palavras"
            },
            {
                role: "user",
                content: "Escreva uma mensagem de uma frase sobre unicornios"
            },
            {
                role: "assistant",
                content: "//mensagem digitada pelo chat"
            },
            {
                role: "user",
                content: "Obrigado"
            },
        ]
    })

    console.log(completion.choices[0].message.content)

}

generateText()


AI em um aplicação real:

import express from 'express';
import OpenAI from 'openai';
import dotenv from "dotenv"
dotenv.config();

const app = express()

const client = new OpenAI({
    apiKey: process.env.OPEN_AI_SECRET_API_KEY
})

app.use(express.json())

app.post("/generate", async (request, response) => {

    const { content } = request.body
    
    try {

        const completion = await client.chat.completions.create({
            model: "gpt-4o-mini",
            max_completion_tokens: 100,
            messages: [
                {
                    role: "developer",
                    content: "User emogis a cada 2 palavras"
                },
                {
                    role: "user",
                    content: content
                },
            ]
        })
    
        response.json({ message: completion.choices[0].message.content})

    } catch(error) {
        response.json({ error: error })
    }
})


export default app

Aqui pegamos um valor enviado no body da aoplicação e mandamos direto para o nosso message no content do user para que atravez dessa messagepossamos retornar algo ao usuario como resposta com um prompt ja definido no developer


Modulo 2 - Structured Outputs e function calling
    Entender o que são saidas estrturadas e usar diferentes ferramentas da OPENAI para gerar saidas estruturadas

Topicos:
    o problema das resposta em texto puro
    Json mode
    Structured outputs

1 - Estruturando dados de output:

    Json Mode -> response_format: { type: "json_object" } --> a resposta vai vim como um json valido

exemplo: 

import { string } from './../node_modules/zod/src/v4/core/regexes';
import express from 'express';
import OpenAI from 'openai';
import dotenv from "dotenv"
dotenv.config();

const app = express()

const client = new OpenAI({
    apiKey: process.env.OPEN_AI_SECRET_API_KEY
})

app.use(express.json())

app.post("/generate", async (request, response) => {

    const { content } = request.body
    
    try {

        const completion = await client.chat.completions.create({
            model: "gpt-4o-mini",
            max_completion_tokens: 100,
            response_format: { type: "json_object" }, --> quero a resposta no formato JSON_OBJECT
            messages: [
                {
                    role: "developer",
                    content: "Liste três produtos que atendam a necessidade do usuario. Responda em JSON no formato { produtos: stroing[] }"
                },
                {
                    role: "user",
                    content: content
                },
            ]
        })

        const output = JSON.parse(completion.choices[0].message.content ?? "") --> transformo a resposta que o chat me der em um json valido e armazeno no output
        
        response.json(output)

    } catch(error) {
        response.json({ error: error })
    }
})


export default app

Structured Output:
    Alem de ser no formato em que pedimo JSON vira tambem na estrutura que pedi

import express from 'express';
import OpenAI from 'openai';
import dotenv from "dotenv"
dotenv.config();
import { z } from "zod"
import { zodResponseFormat } from 'openai/helpers/zod.mjs'; --> importo o zodResponseFormat para que possa fazer a a solicitação do formato que desejo receber a resposta


const app = express()
const client = new OpenAI({
    apiKey: process.env.OPEN_AI_SECRET_API_KEY
})
app.use(express.json())

const schema = z.object({ --> validar se o format que recebi é valido
    produtos: z.array(z.string()) --> quero um objeto com um array de produtos
})

app.post("/generate", async (request, response) => {

    const { content } = request.body
    
    try {

        const completion = await client.chat.completions.parse({ --> altero o client
            model: "gpt-4o-mini",
            max_completion_tokens: 100,
            response_format: zodResponseFormat(schema, "produtos_shema"), --> chamo essa validação --> passo sempre a validação e o nome
            messages: [
                {
                    role: "developer",
                    content: "Liste três produtos que atendam a necessidade do usuario. Responda em JSON no formato { produtos: stroing[] }"
                },
                {
                    role: "user",
                    content: content
                },
            ]
        })

        if (completion.choice[0].message.refusal) { --> caso ele não consiga responder ele cai aqui, por exemplo, passo uma lista de items e minha resposatra não consta nessa lista de resposta ele cai aqui
            response.status(500).json({ refulsal: "refusal error" })
        }

        response.json(completion.choices[0].message.parsed?.produtos) --> ?. valida se o objeto é != null or undefined

    } catch(error) {
        response.json({ error: error })
    }
})


export default app

Function Calling:
    oq é e porque é diferente de prompts tradicionais
    como definir funçoes para serem chamadas pela IA

O que é??
    O dev vai enviar um prompt com a definição das ferramentas que estão disponiveis/funções --> qual a temperatura em paris?? dentro dessas ferramentas vou procurar se tem uma função disponivel para que você busque a temp de algum lugar.
    Viu que tem então ele chama essa função passando o parametro apropriado.
    Essa função sera execuada e nos retornara um result que voltara para o modelo e o modelo gerara a resposta de fato para o user

Ou seja: ela é usada para que possamos chamar funções pré  determinadas no nosso prompt 

throw new Error("Refusal") --> é tipo um return que retorna um erro para o catch direto

TOLLS: --> ferramentas ou funções

tools: [
    {
        type: "function", --> faço a tipagem passando que é uma function
        function: {
            name: "produtosEmEstoque", --> Nome da função
            description: "Lista produtos que estão em estoque", --> descrição da função
            parameters: { --> passo os parametros da função, caso tenha
                type: "object", --> do tipo objeto
                properties: {}, --> aqui é onde eu passo os parametros
                additionalProperties: false --> falo que não tem nenhuma propriedade adicional
            },
            strict: true,
        }
    },
],

tolls_calls:

const { tool_calls } = completion.choices[0].message --> puxa a função que foi reconhecida pelo proprio chat
if (tool_calls) {  --> se tem função
    const [tool_call] = tool_calls  --> pego somente a primeira função
    const toolsMap = { --> Faço o mapeamento de todas as funções 
        produtosEmEstoque: produtosEmEstoque,
        produtosSemEstoque: produtosSemEstoque,
        produtosQuaseSemEstoque: produtosQuaseSemEstoque
    }
    const functionToCall = toolsMap[tool_call.function.name] --> Acho qual a função no map onde melhor se enquadra com o objeto toll_call definido la em cima no tools, onde tem uma function{ name: "", description: ""}
    if (!functionToCall) { se não localizar retorna erro
        throw new Error("Function not found")
    }
    const result = functionToCall() --> localizou então chama a função, nesse caso sem parametros
    const result = functionToCall(toll_call.function.parsed_arguments) --> localizou então chama a função, nesse caso com parametros
}

